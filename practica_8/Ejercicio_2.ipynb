{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejercicio 2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 2 - Cálculo de sueldos\n",
        "\n",
        "Usarémos los datos etiquetados del archivo `datos_empleados_5000.csv`."
      ],
      "metadata": {
        "id": "36g2ZhFQQtcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuraciones"
      ],
      "metadata": {
        "id": "9RDqQtLQ87nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cambiar para usar la configuración correspontiende\n",
        "default_config: int = 1"
      ],
      "metadata": {
        "id": "nU1fY7G3_t-O"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración 1\n",
        "if default_config == 1:\n",
        "    neurons_amnt = 64  # cantidad de neuronas.\n",
        "    kernel_initializer = \"uniform\" # distribución para inicializar pesos.\n",
        "    activation = \"relu\" # función de activación\n",
        "    loss = \"mean_squared_error\" # función de error\n",
        "    learning_rate = 0.1 # cambio respecto al error\n",
        "    batch_size = 10000 # tamaño de bache para el entrenamiento\n",
        "    epochs = 50 # cantidad de épocas\n",
        "\n",
        "# Configuración 2\n",
        "elif default_config == 2:\n",
        "    neurons_amnt = 128  # cantidad de neuronas.\n",
        "    kernel_initializer = \"random_normal\" # distribución para inicializar pesos.\n",
        "    activation = \"relu\" # función de activación\n",
        "    loss = \"mean_squared_error\" # función de error\n",
        "    learning_rate = 0.05 # cambio respecto al error\n",
        "    batch_size = 100 # tamaño de bache para el entrenamiento\n",
        "    epochs = 200 # cantidad de épocas\n",
        "\n",
        "# Configuración 3\n",
        "elif default_config == 3:\n",
        "    neurons_amnt = 32  # cantidad de neuronas.\n",
        "    kernel_initializer = \"uniform\" # distribución para inicializar pesos.\n",
        "    activation = \"relu\" # función de activación\n",
        "    loss = \"mean_squared_error\" # función de error\n",
        "    learning_rate = 0.1 # cambio respecto al error\n",
        "    batch_size = 10000 # tamaño de bache para el entrenamiento\n",
        "    epochs = 15 # cantidad de épocas\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"La variable `default_config` tiene que ser 1, 2, o 3.\")"
      ],
      "metadata": {
        "id": "8sa7BGM18-yR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "Xufpdm6YVaz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "5FvZ-wQsVc6d"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 1: Obtención de datos\n",
        "\n",
        "Cargamos el csv desde el URL del repositorio en un `dataframe` de pandas."
      ],
      "metadata": {
        "id": "lz7eX4UdQ8FQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"https://raw.githubusercontent.com/fdioguardi/UNLP_LeIA/main/practica_8/data/datos_empleados_50000.csv\", index_col=0)\n",
        "data"
      ],
      "metadata": {
        "id": "0rbs3RvbSjXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 2: Preprocesamiento de datos\n",
        "\n",
        "Separamos los datos en:\n",
        "\n",
        "- **x_train**: entrada, conjunto de training.\n",
        "- **y_train**: salida, conjunto de training.\n",
        "- **x_test**: entrada, conjunto de testing.\n",
        "- **x_test**: salida, conjunto de testing.\n",
        "\n",
        "El 80% de los datos se usará para entrenar el modelo, el 20% para testearla."
      ],
      "metadata": {
        "id": "XkLZ9E4ro3ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.get_dummies(data)\n",
        "\n",
        "# la entrada son todos los datos del dataframe excepto por\n",
        "# el campo a predecir.\n",
        "x = data.drop([\"sueldo\"], axis=1)\n",
        "\n",
        "# la salida será solo el dato a predecir.\n",
        "y = np.array(data[\"sueldo\"])\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
      ],
      "metadata": {
        "id": "Xwtzn0YfmVML"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 3: Creación del modelo\n"
      ],
      "metadata": {
        "id": "yRTEcdHhpByf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz-ucr5wF6Yg"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(\n",
        "   Dense(\n",
        "        neurons_amnt,\n",
        "        activation=activation,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        input_shape=(6,)\n",
        "    )\n",
        ")\n",
        "\n",
        "model.add(Dense(neurons_amnt//2, activation=activation))\n",
        "model.add(Dense(1, activation=activation))\n",
        "  \n",
        "model.compile(loss=loss, optimizer=Adam(learning_rate=learning_rate))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 4: Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "gpUHszjhM4Uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, verbose=2, batch_size=batch_size, epochs=epochs)"
      ],
      "metadata": {
        "id": "aEdXh2-5M9ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 5: Evaluación del modelo\n",
        "\n",
        "Usamos el conjunto de test para saber que tan bien funciona el modelo creado."
      ],
      "metadata": {
        "id": "yL6lOQFENF4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXUULrnMNj9v",
        "outputId": "d853e02f-4f4f-4dfa-b50a-cc86a02afb84"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 3821424.7500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3821424.75"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 6: Predicción de nuevos datos\n",
        "\n",
        "Usamos la entrada del conjunto de testeo (que no fue usado para entrenar el modelo) y obtenemos las predicciones outputeadas.\n",
        "\n",
        "Es una versión manual del paso anterior."
      ],
      "metadata": {
        "id": "qVR-iQ8YN0W7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test)\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"Predicción: {y_pred[i]}, Sueldo real: {y_test[i]}\")"
      ],
      "metadata": {
        "id": "W0rIv2UROf71"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}